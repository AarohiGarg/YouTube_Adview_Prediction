# -*- coding: utf-8 -*-
"""Youtube_Adview_Prediction.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MhuasIWPTITX87dERwvxrklfebda3dHp

# Step 1
Import the datasets and libraries, check shape and datatype.
"""

import numpy as np
import pandas as pd
import matplotlib.cm as cm
import matplotlib.pyplot as plt

data_train = pd.read_csv("/train.csv")
data_train.head()
data_train.shape

"""# Step 2
Visualise the dataset using plotting using heatmaps and plots.
"""

# Visualization
# Individual Plots
plt.hist(data_train["category"])
plt.show()
plt.plot(data_train["adview"])
plt.show()

# Remove videos with adview greater than 2000000 as outlier
data_train = data_train[data_train["adview"] <2000000]

# Heatmap
import seaborn as sns
f, ax = plt.subplots(figsize=(10, 8))
corr = data_train.corr()
sns.heatmap(corr, mask=np.zeros_like(corr, dtype=np.bool), cmap=sns.diverging_palette(220, 10, as_cmap=True),
square=True, ax=ax,annot=True)
plt.show()

"""# Step 3
Clean the dataset by removing missing values and other things.
"""

# Removing character "F" present in data
data_train=data_train[data_train.views!='F']
data_train=data_train[data_train.likes!='F']
data_train=data_train[data_train.dislikes!='F']
data_train=data_train[data_train.comment!='F']
data_train.head()

# Assigning each category a number for Category feature
category={'A': 1,'B':2,'C':3,'D':4,'E':5,'F':6,'G':7,'H':8}
data_train["category"]=data_train["category"].map(category)
data_train.head()

"""# Step 4
Transform attributes into numerical values and other necessary transformations
"""

# Convert values to integers for views, likes, comments, dislikes and adview
data_train["views"] = pd.to_numeric(data_train["views"])
data_train["comment"] = pd.to_numeric(data_train["comment"])
data_train["likes"] = pd.to_numeric(data_train["likes"])
data_train["dislikes"] = pd.to_numeric(data_train["dislikes"])
data_train["adview"]=pd.to_numeric(data_train["adview"])
column_vidid=data_train['vidid']

# Endoding features like Category, Duration, Vidid
from sklearn.preprocessing import LabelEncoder
data_train['duration']=LabelEncoder().fit_transform(data_train['duration'])
data_train['vidid']=LabelEncoder().fit_transform(data_train['vidid'])
data_train['published']=LabelEncoder().fit_transform(data_train['published'])
data_train.head()

# Convert Time_in_sec for duration
import datetime
import time
def checki(x):
  y = x[2:]
  h = ''
  m = ''
  s = ''
  mm = ''
  P = ['H','M','S']
  for i in y:
    if i not in P:
      mm+=i
    else:
      if (i=="H"):
        h = mm
        mm = ''
      elif (i == "M"):
        m = mm
        mm = ''
      else:
        s = mm
        mm = ''
  if (h==''):
    h = '00'
  if (m == ''):
    m = '00'
  if (s==''):
    s='00'
  bp = h+':'+m+':'+s
  return bp

train=pd.read_csv("/train.csv")
mp = pd.read_csv("/train.csv")["duration"]
time = mp.apply(checki)
def func_sec(time_string):
  h, m, s = time_string.split(':')
  return int(h) * 3600 + int(m) * 60 + int(s)

time1=time.apply(func_sec)
data_train["duration"]=time1
data_train.head()

"""# Step 5
Normalise your data and split the data into training, validation and test set in the appropriate
ratio.
"""

# Split Data
Y_train = pd.DataFrame(data = data_train.iloc[:, 1].values, columns = ['target'])
data_train=data_train.drop(["adview"],axis=1)
data_train=data_train.drop(["vidid"],axis=1)
data_train.head()
from sklearn.model_selection import train_test_split
X_train, X_test, y_train, y_test = train_test_split(data_train, Y_train, test_size=0.2, random_state=42)
X_train.shape

# Normalise Data
from sklearn.preprocessing import MinMaxScaler
scaler = MinMaxScaler()
X_train=scaler.fit_transform(X_train)
X_test=scaler.fit_transform(X_test)
X_train.mean()

"""# Step 6
Use linear regression, support vector regressor, random forest and for training and get errors.
"""

# Evaluation Metrics
from sklearn import metrics
def print_error(X_test, y_test, model_name):
  prediction = model_name.predict(X_test)
  print('Mean Absolute Error:', metrics.mean_absolute_error(y_test, prediction))
  print('Mean Squared Error:', metrics.mean_squared_error(y_test, prediction))
  print('Root Mean Squared Error:', np.sqrt(metrics.mean_squared_error(y_test, prediction)))

# Linear Regression
from sklearn import linear_model
linear_regression = linear_model.LinearRegression()
linear_regression.fit(X_train, y_train)
print_error(X_test,y_test, linear_regression)

# Support Vector Regressor
from sklearn.svm import SVR
supportvector_regressor = SVR()
supportvector_regressor.fit(X_train,y_train)
print_error(X_test,y_test, supportvector_regressor)

"""# Step 7
Use Decision Tree Regressor and Random Forest Regressors.
"""

# Decision Tree Regressor
from sklearn.tree import DecisionTreeRegressor
decision_tree = DecisionTreeRegressor()
decision_tree.fit(X_train, y_train)
print_error(X_test,y_test, decision_tree)

# Random Forest Regressor
from sklearn.ensemble import RandomForestRegressor
n_estimators = 200
max_depth = 25
min_samples_split=15
min_samples_leaf=2
random_forest = RandomForestRegressor(n_estimators = n_estimators, max_depth = max_depth, min_samples_split=min_samples_split, min_samples_leaf=min_samples_leaf)
random_forest.fit(X_train,y_train)
print_error(X_test,y_test, random_forest)

"""# Step 8
Build an artificial neural network and train it with different layers and
hyperparameters.
"""

# Artificial Neural Network
import keras
from keras.layers import Dense
ann = keras.models.Sequential([
  Dense(6, activation="relu",
  input_shape=X_train.shape[1:]),
  Dense(6,activation="relu"),
  Dense(1)
])
optimizer=keras.optimizers.Adam()
loss=keras.losses.mean_squared_error
ann.compile(optimizer=optimizer,loss=loss,metrics=["mean_squared_error"])
history=ann.fit(X_train,y_train,epochs=100)
ann.summary()
print_error(X_test,y_test,ann)

"""# Step 9
Pick the best model based on error as well as generalisation.
"""

score = linear_regression.score(X_test, y_test)  
print("Test score: {0:.2f} %".format(100 * score))
score = supportvector_regressor.score(X_test, y_test)  
print("Test score: {0:.2f} %".format(100 * score))
score = decision_tree.score(X_test, y_test)  
print("Test score: {0:.2f} %".format(100 * score))
score = random_forest.score(X_test, y_test)  
print("Test score: {0:.2f} %".format(100 * score))

"""# Step 10
Save your model and predict on test set.
"""

#Saving Scikitlearn models
import joblib
joblib.dump(supportvector_regressor, "/supportvector_regressor_youtubeadview.pkl")

"""# Step 11
Preprocess the test data similar to the way train data was preprocessed
"""

data_test = pd.read_csv("/test.csv")

# Removing character "F" present in data
data_test=data_test[data_test.views!='F']
data_test=data_test[data_test.likes!='F']
data_test=data_test[data_test.dislikes!='F']
data_test=data_test[data_test.comment!='F']

# Assigning each category a number for Category feature
category={'A': 1,'B':2,'C':3,'D':4,'E':5,'F':6,'G':7,'H':8}
data_test["category"]=data_test["category"].map(category)

# Convert values to integers for views, likes, comments, dislikes and adview
data_test["views"] = pd.to_numeric(data_test["views"])
data_test["comment"] = pd.to_numeric(data_test["comment"])
data_test["likes"] = pd.to_numeric(data_test["likes"])
data_test["dislikes"] = pd.to_numeric(data_test["dislikes"])

column_vidid=data_test['vidid']

# Endoding features like Category, Duration, Vidid
data_test['duration']=LabelEncoder().fit_transform(data_test['duration'])
data_test['vidid']=LabelEncoder().fit_transform(data_test['vidid'])
data_test['published']=LabelEncoder().fit_transform(data_test['published'])

# Convert Time_in_sec for duration
train=pd.read_csv("/test.csv")
mp = pd.read_csv("/test.csv")["duration"]
time = mp.apply(checki)

time1=time.apply(func_sec)
data_test["duration"]=time1

# Normalise Data
data_testing=data_test.drop(["vidid"],axis=1)
scaler = MinMaxScaler()
X_test = scaler.fit_transform(data_testing)
X_test.shape

"""# Step 12
Use your best model and and preprocessed data to get predictions for
'adview' for each video
"""

# Load from file
joblib_LR_model = joblib.load("/supportvector_regressor_youtubeadview.pkl")

# Predict the Labels using the reloaded Model
Ypredict = joblib_LR_model.predict(X_test)  
Ypredict

"""# Step 13
Save that as "Predictions_Submission.csv"
"""

data_test["Predicted_adview"] = Ypredict
data_test.to_csv("/Predictions_Submission.csv")